{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read big csv file into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 120/120 [10:34<00:00,  5.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from spotify_api import get_multiple_field_information\n",
    "from auth import simple_authenticate\n",
    "\n",
    "def first_or_none(val):\n",
    "    if isinstance(val, (list, tuple)):\n",
    "        print(val[0])\n",
    "        return val[0] if val else None\n",
    "    return val\n",
    "\n",
    "songs_dataframe = pd.read_csv('1_2_m_songs_dataset_deduped.csv')\n",
    "songs_dataframe = songs_dataframe.dropna(subset=['id'])\n",
    "songs_dataframe['artist_ids'] = (\n",
    "    songs_dataframe['artist_ids']\n",
    "      .astype(str)                            # make sure it’s a string\n",
    "      .str.extract(r\"'([^']*)'\")              # grab whatever is inside the first pair of single‐quotes\n",
    "      .iloc[:,0]                              # get the extracted Series\n",
    "      .where(songs_dataframe['artist_ids'].notna())        # put NaNs back where they were\n",
    ")\n",
    "songs_dataframe = songs_dataframe.dropna(subset=['artist_ids'])\n",
    "songs_dataframe = songs_dataframe.drop_duplicates(subset=['id'])\n",
    "\n",
    "bearer_token = simple_authenticate()\n",
    "\n",
    "OUTPUT_CSV = \"1_m_songs.csv\"\n",
    "BATCH_SIZE  = 50        # Spotify \"artists\" endpoint max\n",
    "CHUNK_SIZE  = 10_000    # rows per chunk\n",
    "PAUSE       = 0.1       # seconds between API calls (avoid rate-limit)\n",
    "\n",
    "\n",
    "# Prepare output file (overwrites)\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"track_id\", \"genre\"])\n",
    "\n",
    "# Cache for artist - genre lookups for no double lookups\n",
    "artist_cache = {}\n",
    "\n",
    "n = len(songs_dataframe)\n",
    "\n",
    "# Iterate in row-chunks with a progress bar\n",
    "for start in tqdm(range(0, n, CHUNK_SIZE), desc=\"Processing chunks\"):\n",
    "\n",
    "    chunk = songs_dataframe.iloc[start : start + CHUNK_SIZE]\n",
    "    \n",
    "    # Find which artist_ids we still need to look up\n",
    "    to_lookup = [aid for aid in chunk[\"artist_ids\"].unique() \n",
    "                 if aid not in artist_cache]\n",
    "    \n",
    "    # Call the API in batches of 50\n",
    "    for i in range(0, len(to_lookup), BATCH_SIZE):\n",
    "        batch = to_lookup[i : i + BATCH_SIZE]\n",
    "        resp = get_multiple_field_information(\n",
    "            bearer_token, \n",
    "            \"artists\", \n",
    "            BATCH_SIZE,\n",
    "            *batch\n",
    "        )\n",
    "        # fill cache with missing artist genres combinations\n",
    "        if resp and \"artists\" in resp:\n",
    "            for artist in resp[\"artists\"]:\n",
    "                aid    = artist.get(\"id\")\n",
    "                genres = artist.get(\"genres\") or []\n",
    "                artist_cache[aid] = genres\n",
    "        else:\n",
    "            # on failure or empty, record as no-genres\n",
    "            for aid in batch:\n",
    "                artist_cache[aid] = []\n",
    "        time.sleep(PAUSE)\n",
    "    \n",
    "    # Build output rows for this chunk\n",
    "    rows = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        aid    = row[\"artist_ids\"]\n",
    "        genres = artist_cache.get(aid, [])\n",
    "        if genres:\n",
    "            primary = genres[0]\n",
    "            rows.append([row[\"id\"], primary])\n",
    "        # else: skip any track whose artist has no genres\n",
    "    \n",
    "    # Append to CSV\n",
    "    if rows:\n",
    "        with open(OUTPUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do genre analyis to avoid unneccecary compute time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-genre counts after balancing:\n",
      "genre\n",
      "reggae        2000\n",
      "bluegrass     2000\n",
      "jazz          2000\n",
      "r&b           2000\n",
      "idm           2000\n",
      "folk          2000\n",
      "classical     2000\n",
      "tango         2000\n",
      "grindcore     2000\n",
      "blues         2000\n",
      "punk          2000\n",
      "country       2000\n",
      "soul          2000\n",
      "lo-fi         1303\n",
      "breakbeat      967\n",
      "metal          668\n",
      "rap            612\n",
      "rock           581\n",
      "indie          526\n",
      "pop            431\n",
      "techno         291\n",
      "hip hop        234\n",
      "funk           214\n",
      "house          205\n",
      "hardstyle       85\n",
      "latin           37\n",
      "phonk           13\n",
      "electronic       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Your set of genres\n",
    "popular_genres = {\n",
    "    'country', 'metal', 'heavy-metal', 'hardstyle',\n",
    "    'blues', 'rap', 'hip hop', 'classical', 'folk',\n",
    "    'jazz', 'lo-fi', 'soul', 'punk', 'r&b', 'latin',\n",
    "    'rock', 'rock-n-roll', 'techno', 'pop', 'house', 'phonk',\n",
    "    'indie', 'idm', 'reggae', 'funk', 'acid',\n",
    "    'death-metal', 'bluegrass', 'acoustic', 'electronic',\n",
    "    'tango', 'forro', 'breakbeat', 'grindcore'\n",
    "}\n",
    "\n",
    "# 2) Load your file\n",
    "df = pd.read_csv(\"1_m_songs.csv\")  # columns: track_id, genre\n",
    "\n",
    "# 3) Filter to only those popular genres\n",
    "df = df[df[\"genre\"].isin(popular_genres)]\n",
    "\n",
    "# 4) For each genre, sample up to 2000 rows\n",
    "balanced_parts = []\n",
    "for genre, grp in df.groupby(\"genre\"):\n",
    "    if len(grp) > 2000:\n",
    "        sampled = grp.sample(n=2000, random_state=42)\n",
    "    else:\n",
    "        sampled = grp\n",
    "    balanced_parts.append(sampled)\n",
    "\n",
    "balanced_df = pd.concat(balanced_parts, ignore_index=True)\n",
    "\n",
    "# 5) Optionally shuffle the final DataFrame\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 6) Write out the balanced CSV\n",
    "balanced_df.to_csv(\"track_genres_balanced.csv\", index=False)\n",
    "\n",
    "print(\"Per-genre counts after balancing:\")\n",
    "print(balanced_df[\"genre\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract track features multithreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 194/387 [09:15<08:05,  2.52s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 387/387 [17:49<00:00,  2.76s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing 387 new tracks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 194/2765 [09:37<2:04:26,  2.90s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  14%|█▍        | 395/2765 [18:59<1:40:10,  2.54s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  21%|██▏       | 593/2765 [27:58<1:47:04,  2.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  29%|██▉       | 795/2765 [36:36<1:17:36,  2.36s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  36%|███▌      | 993/2765 [45:09<1:17:01,  2.61s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  43%|████▎     | 1193/2765 [53:41<1:11:39,  2.74s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  51%|█████     | 1398/2765 [1:02:14<41:15,  1.81s/file]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  58%|█████▊    | 1594/2765 [1:10:50<54:32,  2.79s/file]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  65%|██████▍   | 1795/2765 [1:19:24<39:09,  2.42s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  67%|██████▋   | 1857/2765 [1:22:07<42:17,  2.79s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "Processing files:  72%|███████▏  | 1993/2765 [1:27:52<36:48,  2.86s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  74%|███████▍  | 2047/2765 [1:29:55<21:03,  1.76s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=378\n",
      "  warnings.warn(\n",
      "Processing files:  79%|███████▉  | 2193/2765 [1:38:17<27:24,  2.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  87%|████████▋ | 2393/2765 [1:48:18<20:05,  3.24s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  87%|████████▋ | 2416/2765 [1:49:19<13:27,  2.31s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=349\n",
      "  warnings.warn(\n",
      "Processing files:  94%|█████████▍| 2594/2765 [1:59:21<10:49,  3.80s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2765/2765 [2:08:29<00:00,  2.79s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing 2765 new tracks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 196/11243 [11:19<9:26:11,  3.08s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   4%|▎         | 396/11243 [21:36<7:57:56,  2.64s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|▌         | 594/11243 [32:09<10:57:08,  3.70s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 797/11243 [43:51<6:32:49,  2.26s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   9%|▉         | 996/11243 [54:50<6:45:36,  2.37s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  11%|█         | 1200/11243 [1:06:45<10:18:25,  3.69s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "Processing files:  12%|█▏        | 1400/11243 [1:19:05<8:20:16,  3.05s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  14%|█▍        | 1600/11243 [1:33:21<7:07:27,  2.66s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  16%|█▌        | 1793/11243 [1:49:18<11:30:53,  4.39s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  18%|█▊        | 1994/11243 [2:04:37<9:14:27,  3.60s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|█▉        | 2200/11243 [2:14:26<4:58:28,  1.98s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  21%|██▏       | 2399/11243 [2:23:33<4:08:35,  1.69s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  22%|██▏       | 2445/11243 [2:25:45<5:24:04,  2.21s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=305\n",
      "  warnings.warn(\n",
      "Processing files:  23%|██▎       | 2561/11243 [2:31:56<9:26:55,  3.92s/file] /Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=273\n",
      "  warnings.warn(\n",
      "Processing files:  23%|██▎       | 2597/11243 [2:34:03<6:20:01,  2.64s/file] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▍       | 2796/11243 [2:43:18<5:32:20,  2.36s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 2997/11243 [2:51:52<4:50:26,  2.11s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 3041/11243 [2:53:56<7:02:04,  3.09s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=303\n",
      "  warnings.warn(\n",
      "Processing files:  27%|██▋       | 3090/11243 [2:55:59<5:59:37,  2.65s/file]/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=384 is too large for input signal of length=285\n",
      "  warnings.warn(\n",
      "Processing files:  28%|██▊       | 3195/11243 [3:00:26<5:34:11,  2.49s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup created at audio_features_backup.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  30%|██▉       | 3318/11243 [3:05:40<4:01:22,  1.83s/file]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x105e13950>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/agres/Projects/seb/predictify/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Process ForkProcess-44:\n",
      "Process ForkProcess-39:\n",
      "Process ForkProcess-43:\n",
      "Process ForkProcess-38:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-40:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-41:\n",
      "Process ForkProcess-42:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-37:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Processing files:  30%|██▉       | 3320/11243 [3:05:51<7:23:32,  3.36s/file]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(files_to_process), batch_size):\n\u001b[32m     95\u001b[39m     batch = files_to_process[i:i+batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     futures = \u001b[43m[\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_genre\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m     99\u001b[39m         features, genre, file = future.result()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(files_to_process), batch_size):\n\u001b[32m     95\u001b[39m     batch = files_to_process[i:i+batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     futures = [\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_to_genre\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m     99\u001b[39m         features, genre, file = future.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/process.py:791\u001b[39m, in \u001b[36mProcessPoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_lock:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._broken:\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m._broken)\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_thread:\n\u001b[32m    793\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcannot schedule new futures after shutdown\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import librosa\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "## THIS ONLY ON MACOS ## DISABLE ON WINDOWS OR LINUX ##\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "def summarize_feature(feature_array):\n",
    "    means = np.mean(feature_array)\n",
    "    stds = np.std(feature_array)\n",
    "    medians = np.median(feature_array)\n",
    "    return [means, stds, medians] \n",
    "\n",
    "def flatten(track_features):\n",
    "    return [feature for features in track_features for feature in features]\n",
    "\n",
    "def extract_features_librosa(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    features = [\n",
    "        summarize_feature(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)),\n",
    "        summarize_feature(librosa.feature.chroma_stft(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.chroma_cqt(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.chroma_cens(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.chroma_vqt(y=y, sr=sr, intervals='equal')),\n",
    "        summarize_feature(librosa.feature.melspectrogram(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.spectral_bandwidth(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.spectral_contrast(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.spectral_flatness(y=y)),\n",
    "        summarize_feature(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.poly_features(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.tonnetz(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.zero_crossing_rate(y)),\n",
    "        summarize_feature(librosa.feature.tempogram(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.fourier_tempogram(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.tempogram_ratio(y=y, sr=sr)),\n",
    "        summarize_feature(librosa.feature.rms(y=y)),\n",
    "        np.ravel(librosa.feature.tempo(y=y, sr=sr))\n",
    "    ]\n",
    "    return flatten(features)\n",
    "\n",
    "def process_file(file, folder_path, id_to_genre):\n",
    "    try:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        file_id = os.path.splitext(file)[0]\n",
    "        features = extract_features_librosa(file_path)\n",
    "        genre = id_to_genre.get(file_id, None)\n",
    "        return features, genre, file\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None, None, file\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Load the CSV file with track IDs and genres\n",
    "    tracks_info_df = pd.read_csv(\"track_genre_balanced_url.csv\")\n",
    "\n",
    "    # Load pickle\n",
    "    results_file = 'audio_features.pkl'\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'rb') as file:\n",
    "            saved_data = pickle.load(file)\n",
    "        X = saved_data.get('X', [])\n",
    "        y_labels = saved_data.get('y_labels', [])\n",
    "        processed_files = set(saved_data.get('processed_files', []))\n",
    "    else:\n",
    "        X = []\n",
    "        y_labels = []\n",
    "        processed_files = set()\n",
    "\n",
    "    # Get the list of all files in the folder and filter out the processed ones\n",
    "    folder_path = './audio_previews'\n",
    "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.mp3')]\n",
    "    files_to_process = [f for f in all_files if f not in processed_files]\n",
    "\n",
    "    # Create a mapping from track_id to genre\n",
    "    id_to_genre = dict(zip(tracks_info_df['track_id'], tracks_info_df['genre']))\n",
    "\n",
    "    # Define some constants\n",
    "    batch_size = os.cpu_count() or 4  # Use all available CPU cores but one or 4 if none available\n",
    "    backup_interval = 100\n",
    "    counter = 0\n",
    "\n",
    "    with tqdm(total=len(files_to_process), desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "        with ProcessPoolExecutor(max_workers=batch_size) as executor:\n",
    "            futures = []\n",
    "            for i in range(0, len(files_to_process), batch_size):\n",
    "                batch = files_to_process[i:i+batch_size]\n",
    "                futures = [executor.submit(process_file, file, folder_path, id_to_genre) for file in batch]\n",
    "\n",
    "                for future in futures:\n",
    "                    features, genre, file = future.result()\n",
    "                    if features is not None:\n",
    "                        X.append(features)\n",
    "                        y_labels.append(genre)\n",
    "                        processed_files.add(file)\n",
    "                        counter += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "                # Save after each batch\n",
    "                with open(results_file, 'wb') as f:\n",
    "                    pickle.dump({\n",
    "                        'X': X,\n",
    "                        'y_labels': y_labels,\n",
    "                        'processed_files': list(processed_files)\n",
    "                    }, f)\n",
    "\n",
    "                if counter % backup_interval == 0:\n",
    "                    backup_file = results_file.replace(\".pkl\", \"_backup.pkl\")\n",
    "                    shutil.copy(results_file, backup_file)\n",
    "                    print(f\"Backup created at {backup_file}\")\n",
    "\n",
    "    print(f\"Done processing {counter} new tracks.\")\n",
    "\n",
    "    time.sleep(900) # Sleep for 15 minutes before the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two pkl files\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def merge_pickles(file1, file2, output_file):\n",
    "    with open(file1, 'rb') as f1, open(file2, 'rb') as f2:\n",
    "        data1 = pickle.load(f1)\n",
    "        data2 = pickle.load(f2)\n",
    "\n",
    "    # Merge the data\n",
    "    merged_data = {\n",
    "        'X': data1['X'] + data2['X'],\n",
    "        'y_labels': data1['y_labels'] + data2['y_labels'],\n",
    "        'processed_files': list(set(data1['processed_files'] + data2['processed_files']))\n",
    "    }\n",
    "\n",
    "    # Save the merged data\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        pickle.dump(merged_data, f_out)\n",
    "\n",
    "# Usage\n",
    "file1 = 'audio_features.pkl'\n",
    "file2 = 'audio_features_chris.pkl'\n",
    "output_file = 'audio_features_merged.pkl'\n",
    "\n",
    "merge_pickles(file1, file2, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
